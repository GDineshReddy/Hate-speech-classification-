{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d34ccbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_excel(\"outpute.xlsx\")\n",
    "\n",
    "# Tokenize the text\n",
    "data[\"Tokenized Text\"] = data[\"Cleaned Text\"].apply(word_tokenize)\n",
    "\n",
    "# Save the preprocessed dataset\n",
    "data.to_excel(\"preprocessed_output.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26fb6e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dinesh Reddy\\AppData\\Local\\Temp\\ipykernel_10604\\6728923.py:18: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  glove2word2vec(glove_input_file, word2vec_output_file)\n"
     ]
    }
   ],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load the preprocessed dataset\n",
    "data = pd.read_excel(\"preprocessed_output.xlsx\")\n",
    "\n",
    "# Train GloVe embeddings\n",
    "sentences = data[\"Tokenized Text\"].tolist()\n",
    "\n",
    "# Save tokenized sentences to a text file\n",
    "with open(\"sentences.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    for sentence in sentences:\n",
    "        file.write(\" \".join(sentence) + \"\\n\")\n",
    "\n",
    "# Convert GloVe format to Word2Vec format\n",
    "glove_input_file = \"glove.6B.100d.txt\"\n",
    "word2vec_output_file = \"glove.6B.100d.word2vec.txt\"\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "\n",
    "# Load the converted GloVe embeddings\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n",
    "\n",
    "# Save the trained embeddings\n",
    "model.save(\"glove_model.bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20a5b348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "59/59 [==============================] - 7s 62ms/step - loss: 0.6698 - accuracy: 0.6005 - val_loss: 0.6075 - val_accuracy: 0.7042\n",
      "Epoch 2/10\n",
      "59/59 [==============================] - 3s 53ms/step - loss: 0.4335 - accuracy: 0.8159 - val_loss: 0.4280 - val_accuracy: 0.8017\n",
      "Epoch 3/10\n",
      "59/59 [==============================] - 3s 56ms/step - loss: 0.2662 - accuracy: 0.9051 - val_loss: 0.4573 - val_accuracy: 0.7867\n",
      "Epoch 4/10\n",
      "59/59 [==============================] - 3s 56ms/step - loss: 0.1495 - accuracy: 0.9563 - val_loss: 0.5037 - val_accuracy: 0.8006\n",
      "Epoch 5/10\n",
      "59/59 [==============================] - 3s 55ms/step - loss: 0.0892 - accuracy: 0.9740 - val_loss: 0.5537 - val_accuracy: 0.8049\n",
      "Epoch 6/10\n",
      "59/59 [==============================] - 3s 57ms/step - loss: 0.0737 - accuracy: 0.9783 - val_loss: 0.6100 - val_accuracy: 0.7931\n",
      "Epoch 7/10\n",
      "59/59 [==============================] - 3s 55ms/step - loss: 0.0596 - accuracy: 0.9812 - val_loss: 0.6594 - val_accuracy: 0.7889\n",
      "Epoch 8/10\n",
      "59/59 [==============================] - 3s 54ms/step - loss: 0.0544 - accuracy: 0.9818 - val_loss: 0.6962 - val_accuracy: 0.8006\n",
      "Epoch 9/10\n",
      "59/59 [==============================] - 3s 57ms/step - loss: 0.0525 - accuracy: 0.9834 - val_loss: 0.7514 - val_accuracy: 0.7835\n",
      "Epoch 10/10\n",
      "59/59 [==============================] - 4s 61ms/step - loss: 0.0505 - accuracy: 0.9823 - val_loss: 0.8398 - val_accuracy: 0.7835\n",
      "30/30 [==============================] - 1s 11ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.87      0.79       441\n",
      "           1       0.86      0.70      0.77       492\n",
      "\n",
      "    accuracy                           0.78       933\n",
      "   macro avg       0.79      0.79      0.78       933\n",
      "weighted avg       0.80      0.78      0.78       933\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "\n",
    "# Load the preprocessed dataset\n",
    "data = pd.read_excel(\"preprocessed_output.xlsx\")\n",
    "\n",
    "# Convert lists of strings to sequences of integers\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data[\"Tokenized Text\"])\n",
    "sequences = tokenizer.texts_to_sequences(data[\"Tokenized Text\"])\n",
    "\n",
    "# Pad sequences to a fixed length\n",
    "max_sequence_length = 100\n",
    "X = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "y = data[\"task_1\"].apply(lambda x: 1 if x == \"HOF\" else 0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the CNN model\n",
    "embedding_dim = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim, input_length=max_sequence_length))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test))\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.round(y_pred).flatten()\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a42aa9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "59/59 [==============================] - 66s 937ms/step - loss: 0.6460 - accuracy: 0.6249 - val_loss: 0.5943 - val_accuracy: 0.6795\n",
      "Epoch 2/10\n",
      "59/59 [==============================] - 54s 917ms/step - loss: 0.3745 - accuracy: 0.8365 - val_loss: 0.4409 - val_accuracy: 0.7846\n",
      "Epoch 3/10\n",
      "59/59 [==============================] - 23s 398ms/step - loss: 0.2458 - accuracy: 0.9059 - val_loss: 0.5117 - val_accuracy: 0.7985\n",
      "Epoch 4/10\n",
      "59/59 [==============================] - 24s 411ms/step - loss: 0.1680 - accuracy: 0.9416 - val_loss: 0.5240 - val_accuracy: 0.7867\n",
      "Epoch 5/10\n",
      "59/59 [==============================] - 25s 417ms/step - loss: 0.1197 - accuracy: 0.9579 - val_loss: 0.7356 - val_accuracy: 0.7760\n",
      "Epoch 6/10\n",
      "59/59 [==============================] - 22s 364ms/step - loss: 0.0767 - accuracy: 0.9737 - val_loss: 0.7254 - val_accuracy: 0.7749\n",
      "Epoch 7/10\n",
      "59/59 [==============================] - 24s 408ms/step - loss: 0.0649 - accuracy: 0.9770 - val_loss: 0.8653 - val_accuracy: 0.7663\n",
      "Epoch 8/10\n",
      "59/59 [==============================] - 25s 418ms/step - loss: 0.0592 - accuracy: 0.9796 - val_loss: 0.8206 - val_accuracy: 0.7653\n",
      "Epoch 9/10\n",
      "59/59 [==============================] - 25s 417ms/step - loss: 0.0578 - accuracy: 0.9794 - val_loss: 0.9598 - val_accuracy: 0.7663\n",
      "Epoch 10/10\n",
      "59/59 [==============================] - 25s 421ms/step - loss: 0.0531 - accuracy: 0.9807 - val_loss: 1.1927 - val_accuracy: 0.7706\n",
      "30/30 [==============================] - 1s 26ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.80      0.77       441\n",
      "           1       0.81      0.74      0.77       492\n",
      "\n",
      "    accuracy                           0.77       933\n",
      "   macro avg       0.77      0.77      0.77       933\n",
      "weighted avg       0.77      0.77      0.77       933\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# Load the preprocessed dataset\n",
    "data = pd.read_excel(\"preprocessed_output.xlsx\")\n",
    "\n",
    "# Convert lists of strings to sequences of integers\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data[\"Tokenized Text\"])\n",
    "sequences = tokenizer.texts_to_sequences(data[\"Tokenized Text\"])\n",
    "\n",
    "# Pad sequences to a fixed length\n",
    "max_sequence_length = 100\n",
    "X = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "y = data[\"task_1\"].apply(lambda x: 1 if x == \"HOF\" else 0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the LSTM model\n",
    "embedding_dim = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim, input_length=max_sequence_length))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test))\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.round(y_pred).flatten()\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b6a250a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "59/59 [==============================] - 76s 1s/step - loss: 0.6338 - accuracy: 0.6318 - val_loss: 0.4926 - val_accuracy: 0.7674\n",
      "Epoch 2/10\n",
      "59/59 [==============================] - 70s 1s/step - loss: 0.3553 - accuracy: 0.8574 - val_loss: 0.4373 - val_accuracy: 0.8060\n",
      "Epoch 3/10\n",
      "59/59 [==============================] - 74s 1s/step - loss: 0.2332 - accuracy: 0.9137 - val_loss: 0.6561 - val_accuracy: 0.7889\n",
      "Epoch 4/10\n",
      "59/59 [==============================] - 75s 1s/step - loss: 0.1524 - accuracy: 0.9483 - val_loss: 0.6302 - val_accuracy: 0.7814\n",
      "Epoch 5/10\n",
      "59/59 [==============================] - 76s 1s/step - loss: 0.1154 - accuracy: 0.9617 - val_loss: 0.7926 - val_accuracy: 0.7663\n",
      "Epoch 6/10\n",
      "59/59 [==============================] - 76s 1s/step - loss: 0.0929 - accuracy: 0.9692 - val_loss: 0.9304 - val_accuracy: 0.7738\n",
      "Epoch 7/10\n",
      "59/59 [==============================] - 76s 1s/step - loss: 0.0790 - accuracy: 0.9716 - val_loss: 0.9707 - val_accuracy: 0.7556\n",
      "Epoch 8/10\n",
      "59/59 [==============================] - 240s 4s/step - loss: 0.0769 - accuracy: 0.9713 - val_loss: 0.9831 - val_accuracy: 0.7567\n",
      "Epoch 9/10\n",
      "59/59 [==============================] - 79s 1s/step - loss: 0.0624 - accuracy: 0.9780 - val_loss: 1.0488 - val_accuracy: 0.7610\n",
      "Epoch 10/10\n",
      "59/59 [==============================] - 80s 1s/step - loss: 0.0584 - accuracy: 0.9780 - val_loss: 0.9938 - val_accuracy: 0.7471\n",
      "30/30 [==============================] - 2s 53ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.71      0.73       441\n",
      "           1       0.75      0.78      0.77       492\n",
      "\n",
      "    accuracy                           0.75       933\n",
      "   macro avg       0.75      0.75      0.75       933\n",
      "weighted avg       0.75      0.75      0.75       933\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
    "\n",
    "# Load the preprocessed dataset\n",
    "data = pd.read_excel(\"preprocessed_output.xlsx\")\n",
    "\n",
    "# Convert lists of strings to sequences of integers\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data[\"Tokenized Text\"])\n",
    "sequences = tokenizer.texts_to_sequences(data[\"Tokenized Text\"])\n",
    "\n",
    "# Pad sequences to a fixed length\n",
    "max_sequence_length = 100\n",
    "X = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "y = data[\"task_1\"].apply(lambda x: 1 if x == \"HOF\" else 0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the BiLSTM model\n",
    "embedding_dim = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim, input_length=max_sequence_length))\n",
    "model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test))\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.round(y_pred).flatten()\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3073860",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c48bc33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d010c9fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288ce23b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de7ad04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
