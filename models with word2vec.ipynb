{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea584f08",
   "metadata": {},
   "source": [
    "# Required libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "001b4034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_excel(\"outpute.xlsx\", sheet_name=\"Sheet1\")\n",
    "\n",
    "# Preprocess the text\n",
    "preprocessed_text = data[\"Cleaned Text\"].apply(lambda x: simple_preprocess(x))\n",
    "\n",
    "# Train Word2Vec embeddings\n",
    "model = Word2Vec(sentences=preprocessed_text, vector_size=100, window=5, min_count=1, workers=4, sg=0)\n",
    "\n",
    "# Save the embeddings\n",
    "model.save(\"word2vec_model.bin\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb876e62",
   "metadata": {},
   "source": [
    "# Word2vec embeddings and cnn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c47857f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "234/234 [==============================] - 3s 7ms/step - loss: 0.6864 - accuracy: 0.5707\n",
      "Epoch 2/10\n",
      "234/234 [==============================] - 2s 7ms/step - loss: 0.6817 - accuracy: 0.5807\n",
      "Epoch 3/10\n",
      "234/234 [==============================] - 2s 7ms/step - loss: 0.6555 - accuracy: 0.6112\n",
      "Epoch 4/10\n",
      "234/234 [==============================] - 2s 7ms/step - loss: 0.6504 - accuracy: 0.6184\n",
      "Epoch 5/10\n",
      "234/234 [==============================] - 2s 7ms/step - loss: 0.6474 - accuracy: 0.6316\n",
      "Epoch 6/10\n",
      "234/234 [==============================] - 2s 7ms/step - loss: 0.6462 - accuracy: 0.6254\n",
      "Epoch 7/10\n",
      "234/234 [==============================] - 2s 7ms/step - loss: 0.6417 - accuracy: 0.6434\n",
      "Epoch 8/10\n",
      "234/234 [==============================] - 2s 7ms/step - loss: 0.6393 - accuracy: 0.6369\n",
      "Epoch 9/10\n",
      "234/234 [==============================] - 2s 7ms/step - loss: 0.6419 - accuracy: 0.6412\n",
      "Epoch 10/10\n",
      "234/234 [==============================] - 2s 7ms/step - loss: 0.6341 - accuracy: 0.6455\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.7524 - accuracy: 0.5005\n",
      "Test Loss: 0.7523726224899292\n",
      "Test Accuracy: 0.5005359053611755\n",
      "30/30 [==============================] - 0s 4ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.71      0.38       201\n",
      "           1       0.85      0.44      0.58       732\n",
      "\n",
      "    accuracy                           0.50       933\n",
      "   macro avg       0.55      0.58      0.48       933\n",
      "weighted avg       0.72      0.50      0.54       933\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the Word2Vec embeddings\n",
    "word2vec_model = Word2Vec.load(\"word2vec_model.bin\")\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_excel(\"outpute.xlsx\", sheet_name=\"Sheet1\")\n",
    "\n",
    "# Preprocess the text\n",
    "preprocessed_text = data[\"Cleaned Text\"].apply(lambda x: simple_preprocess(x))\n",
    "\n",
    "# Convert text to numerical sequences\n",
    "sequences = []\n",
    "for text in preprocessed_text:\n",
    "    sequence = [word2vec_model.wv.key_to_index[word] for word in text if word in word2vec_model.wv.key_to_index]\n",
    "    sequences.append(sequence)\n",
    "\n",
    "# Determine the maximum sequence length\n",
    "max_length = max(len(seq) for seq in sequences)\n",
    "\n",
    "# Pad sequences\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length)\n",
    "\n",
    "# Prepare labels\n",
    "labels = (data[\"task_1\"] == \"HOF\").astype(int)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(padded_sequences) * split_ratio)\n",
    "\n",
    "x_train, x_test = padded_sequences[:split_index], padded_sequences[split_index:]\n",
    "y_train, y_test = labels[:split_index], labels[split_index:]\n",
    "\n",
    "# Define the CNN model\n",
    "embedding_dim = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(word2vec_model.wv.key_to_index), output_dim=embedding_dim, weights=[word2vec_model.wv.vectors], trainable=False))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "epochs = 10\n",
    "batch_size = 16\n",
    "\n",
    "model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(x_test)\n",
    "\n",
    "# Classify the predictions as HOF or NOT based on a threshold\n",
    "threshold = 0.5\n",
    "classified_predictions = [1 if pred >= threshold else 0 for pred in predictions]\n",
    "\n",
    "# Calculate classification scores\n",
    "classification_scores = classification_report(y_test, classified_predictions)\n",
    "print(classification_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d79bcd8",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ea1388c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "234/234 [==============================] - 8s 24ms/step - loss: 0.6878 - accuracy: 0.5528\n",
      "Epoch 2/10\n",
      "234/234 [==============================] - 5s 23ms/step - loss: 0.6560 - accuracy: 0.6069\n",
      "Epoch 3/10\n",
      "234/234 [==============================] - 5s 23ms/step - loss: 0.6388 - accuracy: 0.6313\n",
      "Epoch 4/10\n",
      "234/234 [==============================] - 5s 23ms/step - loss: 0.6299 - accuracy: 0.6546\n",
      "Epoch 5/10\n",
      "234/234 [==============================] - 5s 23ms/step - loss: 0.6260 - accuracy: 0.6509\n",
      "Epoch 6/10\n",
      "234/234 [==============================] - 5s 23ms/step - loss: 0.6218 - accuracy: 0.6533\n",
      "Epoch 7/10\n",
      "234/234 [==============================] - 6s 24ms/step - loss: 0.6142 - accuracy: 0.6675\n",
      "Epoch 8/10\n",
      "234/234 [==============================] - 5s 23ms/step - loss: 0.6153 - accuracy: 0.6635\n",
      "Epoch 9/10\n",
      "234/234 [==============================] - 6s 24ms/step - loss: 0.6117 - accuracy: 0.6699\n",
      "Epoch 10/10\n",
      "234/234 [==============================] - 6s 24ms/step - loss: 0.6072 - accuracy: 0.6755\n",
      "30/30 [==============================] - 1s 12ms/step - loss: 0.6999 - accuracy: 0.5809\n",
      "Test Loss: 0.6998701095581055\n",
      "Test Accuracy: 0.5809217691421509\n",
      "30/30 [==============================] - 1s 11ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.64      0.40       201\n",
      "           1       0.85      0.56      0.68       732\n",
      "\n",
      "    accuracy                           0.58       933\n",
      "   macro avg       0.57      0.60      0.54       933\n",
      "weighted avg       0.73      0.58      0.62       933\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the Word2Vec embeddings\n",
    "word2vec_model = Word2Vec.load(\"word2vec_model.bin\")\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_excel(\"outpute.xlsx\", sheet_name=\"Sheet1\")\n",
    "\n",
    "# Preprocess the text\n",
    "preprocessed_text = data[\"Cleaned Text\"].apply(lambda x: simple_preprocess(x))\n",
    "\n",
    "# Convert text to numerical sequences\n",
    "sequences = []\n",
    "for text in preprocessed_text:\n",
    "    sequence = [word2vec_model.wv.key_to_index[word] for word in text if word in word2vec_model.wv.key_to_index]\n",
    "    sequences.append(sequence)\n",
    "\n",
    "# Determine the maximum sequence length\n",
    "max_length = max(len(seq) for seq in sequences)\n",
    "\n",
    "# Pad sequences\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length)\n",
    "\n",
    "# Prepare labels\n",
    "labels = (data[\"task_1\"] == \"HOF\").astype(int)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(padded_sequences) * split_ratio)\n",
    "\n",
    "x_train, x_test = padded_sequences[:split_index], padded_sequences[split_index:]\n",
    "y_train, y_test = labels[:split_index], labels[split_index:]\n",
    "\n",
    "# Define the LSTM model\n",
    "embedding_dim = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(word2vec_model.wv.key_to_index), output_dim=embedding_dim, weights=[word2vec_model.wv.vectors], trainable=False))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "epochs = 10\n",
    "batch_size = 16\n",
    "\n",
    "model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(x_test)\n",
    "\n",
    "# Classify the predictions as HOF or NOT based on a threshold\n",
    "threshold = 0.5\n",
    "classified_predictions = [1 if pred >= threshold else 0 for pred in predictions]\n",
    "\n",
    "# Calculate classification scores\n",
    "classification_scores = classification_report(y_test, classified_predictions)\n",
    "print(classification_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c328c4b",
   "metadata": {},
   "source": [
    "# Bi-LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55863441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "234/234 [==============================] - 8s 26ms/step - loss: 0.6783 - accuracy: 0.5702\n",
      "Epoch 2/10\n",
      "234/234 [==============================] - 7s 29ms/step - loss: 0.6445 - accuracy: 0.6270\n",
      "Epoch 3/10\n",
      "234/234 [==============================] - 6s 27ms/step - loss: 0.6348 - accuracy: 0.6407\n",
      "Epoch 4/10\n",
      "234/234 [==============================] - 7s 29ms/step - loss: 0.6317 - accuracy: 0.6565\n",
      "Epoch 5/10\n",
      "234/234 [==============================] - 7s 29ms/step - loss: 0.6326 - accuracy: 0.6404\n",
      "Epoch 6/10\n",
      "234/234 [==============================] - 7s 28ms/step - loss: 0.6296 - accuracy: 0.6474\n",
      "Epoch 7/10\n",
      "234/234 [==============================] - 7s 29ms/step - loss: 0.6201 - accuracy: 0.6586\n",
      "Epoch 8/10\n",
      "234/234 [==============================] - 7s 29ms/step - loss: 0.6159 - accuracy: 0.6645\n",
      "Epoch 9/10\n",
      "234/234 [==============================] - 6s 27ms/step - loss: 0.6100 - accuracy: 0.6685\n",
      "Epoch 10/10\n",
      "234/234 [==============================] - 7s 28ms/step - loss: 0.6103 - accuracy: 0.6702\n",
      "30/30 [==============================] - 1s 11ms/step - loss: 0.7095 - accuracy: 0.5713\n",
      "Test Loss: 0.7094604969024658\n",
      "Test Accuracy: 0.5712754726409912\n",
      "30/30 [==============================] - 1s 11ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.62      0.38       201\n",
      "           1       0.84      0.56      0.67       732\n",
      "\n",
      "    accuracy                           0.57       933\n",
      "   macro avg       0.56      0.59      0.53       933\n",
      "weighted avg       0.72      0.57      0.61       933\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the Word2Vec embeddings\n",
    "word2vec_model = Word2Vec.load(\"word2vec_model.bin\")\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_excel(\"outpute.xlsx\", sheet_name=\"Sheet1\")\n",
    "\n",
    "# Preprocess the text\n",
    "preprocessed_text = data[\"Cleaned Text\"].apply(lambda x: simple_preprocess(x))\n",
    "\n",
    "# Convert text to numerical sequences\n",
    "sequences = []\n",
    "for text in preprocessed_text:\n",
    "    sequence = [word2vec_model.wv.key_to_index[word] for word in text if word in word2vec_model.wv.key_to_index]\n",
    "    sequences.append(sequence)\n",
    "\n",
    "# Determine the maximum sequence length\n",
    "max_length = max(len(seq) for seq in sequences)\n",
    "\n",
    "# Pad sequences\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length)\n",
    "\n",
    "# Prepare labels\n",
    "labels = (data[\"task_1\"] == \"HOF\").astype(int)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(padded_sequences) * split_ratio)\n",
    "\n",
    "x_train, x_test = padded_sequences[:split_index], padded_sequences[split_index:]\n",
    "y_train, y_test = labels[:split_index], labels[split_index:]\n",
    "\n",
    "# Define the Bi-LSTM model\n",
    "embedding_dim = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(word2vec_model.wv.key_to_index), output_dim=embedding_dim, weights=[word2vec_model.wv.vectors], trainable=False))\n",
    "model.add(Bidirectional(LSTM(128)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "epochs = 10\n",
    "batch_size = 16\n",
    "\n",
    "model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(x_test)\n",
    "\n",
    "# Classify the predictions as HOF or NOT based on a threshold\n",
    "threshold = 0.5\n",
    "classified_predictions = [1 if pred >= threshold else 0 for pred in predictions]\n",
    "\n",
    "# Calculate classification scores\n",
    "classification_scores = classification_report(y_test, classified_predictions)\n",
    "print(classification_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7655d6a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
