{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6ba5d85",
   "metadata": {},
   "source": [
    "# Required Libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e558973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of             text_id                                               text task_1  \\\n",
       "0     hasoc_hi_5556  बांग्लादेश की शानदार वापसी, भारत को 314 रन पर ...    NOT   \n",
       "1     hasoc_hi_5648  सब रंडी नाच देखने मे व्यस्त जैसे ही कोई #शांती...    HOF   \n",
       "2      hasoc_hi_164  तुम जैसे हरामियों के लिए बस जूतों की कमी है शु...    HOF   \n",
       "3     hasoc_hi_3530  बीजेपी  आकाश विजयवर्गीय जेल से रिहा, जमानत मिल...    NOT   \n",
       "4     hasoc_hi_5206  चमकी बुखार: विधानसभा परिसर में आरजेडी का प्रदर...    NOT   \n",
       "...             ...                                                ...    ...   \n",
       "4660  hasoc_hi_6606  पाकिस्तान ने हिंदुओं के ख़िलाफ़ बोलने वाले को ...    NOT   \n",
       "4661  hasoc_hi_4931  कोहली है #नेहरू नहीं जो अंग्रेजों के तलवे चाटन...    HOF   \n",
       "4662  hasoc_hi_1059       परशुराम? वही जिसने अपनी मां की हत्या की थीं?    NOT   \n",
       "4663  hasoc_hi_5429  जिस देश में #कन्हैया_कुमार जैसा पढ़ा लिखा युवा...    HOF   \n",
       "4664  hasoc_hi_1656  इनके बापों मैं भी दम नहीं जो भारत को इस्लामिक ...    HOF   \n",
       "\n",
       "     task_2 task_3                                       Cleaned Text  \\\n",
       "0      NONE   NONE            बगलदश क शनदर वपस भरत क 314 रन पर रक  19   \n",
       "1      PRFN    UNT  सब रड नच दखन म वयसत जस ह कई शतदत क सथ कछ हग सब...   \n",
       "2      PRFN    TIN  तम जस हरमय क लए बस जत क कम ह शकर कर अभ तमहर लच...   \n",
       "3      NONE   NONE  बजप  आकश वजयवरगय जल स रह जमनत मलन क खश म एक सम...   \n",
       "4      NONE   NONE  चमक बखर वधनसभ परसर म आरजड क परदरशन तजसव यदव नद...   \n",
       "...     ...    ...                                                ...   \n",
       "4660   NONE   NONE  पकसतन न हदओ क खलफ बलन वल क बरखसत कय ह  कय भरत ...   \n",
       "4661   PRFN    TIN      कहल ह नहर नह ज अगरज क तलव चटन लग  भडव सल जवहर   \n",
       "4662   NONE   NONE                       परशरम वह जसन अपन म क हतय क थ   \n",
       "4663   HATE    TIN  जस दश म कनहय_कमर जस पढ लख यव हर जए आतकवद  जत ज...   \n",
       "4664   HATE    TIN  इनक बप म भ दम नह ज भरत क इसलमक सटट बन सक हमर भ...   \n",
       "\n",
       "                                         Tokenized Text  \n",
       "0     [बगलदश, क, शनदर, वपस, भरत, क, 314, रन, पर, रक,...  \n",
       "1     [सब, रड, नच, दखन, म, वयसत, जस, ह, कई, शतदत, क,...  \n",
       "2     [तम, जस, हरमय, क, लए, बस, जत, क, कम, ह, शकर, क...  \n",
       "3     [बजप, आकश, वजयवरगय, जल, स, रह, जमनत, मलन, क, ख...  \n",
       "4     [चमक, बखर, वधनसभ, परसर, म, आरजड, क, परदरशन, तज...  \n",
       "...                                                 ...  \n",
       "4660  [पकसतन, न, हदओ, क, खलफ, बलन, वल, क, बरखसत, कय,...  \n",
       "4661  [कहल, ह, नहर, नह, ज, अगरज, क, तलव, चटन, लग, भड...  \n",
       "4662             [परशरम, वह, जसन, अपन, म, क, हतय, क, थ]  \n",
       "4663  [जस, दश, म, कनहय_कमर, जस, पढ, लख, यव, हर, जए, ...  \n",
       "4664  [इनक, बप, म, भ, दम, नह, ज, भरत, क, इसलमक, सटट,...  \n",
       "\n",
       "[4665 rows x 7 columns]>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_excel(\"outpute.xlsx\")\n",
    "\n",
    "# Tokenize the text\n",
    "data[\"Tokenized Text\"] = data[\"Cleaned Text\"].apply(word_tokenize)\n",
    "\n",
    "# Save the preprocessed dataset\n",
    "data.to_excel(\"preprocessed_output.xlsx\", index=False)\n",
    "\n",
    "data.head\n"
   ]
  },
  
  
  {
   "cell_type": "markdown",
   "id": "afada2e2",
   "metadata": {},
   "source": [
    "# CNN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f8646d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"preprocessed_output.xlsx\")                             # Load the preprocessed dataset\n",
    "\n",
    "tokenizer = Tokenizer()                                                      # Convert lists of strings to sequences of integers\n",
    "tokenizer.fit_on_texts(data[\"Tokenized Text\"])\n",
    "sequences = tokenizer.texts_to_sequences(data[\"Tokenized Text\"])\n",
    "\n",
    "max_sequence_length = 100                                                    # Pad sequences to a fixed length\n",
    "X = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "y = data[\"task_1\"].apply(lambda x: 1 if x == \"HOF\" else 0)                  # Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "embedding_dim = 100                                                        # Define the CNN model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim, input_length=max_sequence_length))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test))  # Train the model\n",
    " \n",
    "y_pred = model.predict(X_test)                                                           # Make predictions\n",
    "y_pred = np.round(y_pred).flatten()\n",
    "\n",
    "report = classification_report(y_test, y_pred)                                          # Calculate evaluation metrics\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224f5737",
   "metadata": {},
   "source": [
    "# LSTM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b14f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"preprocessed_output.xlsx\")                                     # Load the preprocessed dataset\n",
    "\n",
    "tokenizer = Tokenizer()                                                     # Convert lists of strings to sequences of integers\n",
    "tokenizer.fit_on_texts(data[\"Tokenized Text\"])\n",
    "sequences = tokenizer.texts_to_sequences(data[\"Tokenized Text\"])\n",
    "\n",
    "max_sequence_length = 100                                                  # Pad sequences to a fixed length\n",
    "X = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "y = data[\"task_1\"].apply(lambda x: 1 if x == \"HOF\" else 0)                # Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "embedding_dim = 100                                                      # Define the LSTM model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim, input_length=max_sequence_length))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test))     # Train the model\n",
    "\n",
    "y_pred = model.predict(X_test)                                                             # Make predictions\n",
    "y_pred = np.round(y_pred).flatten()\n",
    "\n",
    "report = classification_report(y_test, y_pred)                                            # Calculate evaluation metrics\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80bb525",
   "metadata": {},
   "source": [
    "# BI-LSTM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6a3e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"preprocessed_output.xlsx\")                                    # Load the preprocessed dataset\n",
    "\n",
    "tokenizer = Tokenizer()                                                     # Convert lists of strings to sequences of integers\n",
    "tokenizer.fit_on_texts(data[\"Tokenized Text\"])\n",
    "sequences = tokenizer.texts_to_sequences(data[\"Tokenized Text\"])\n",
    "\n",
    "max_sequence_length = 100                                                  # Pad sequences to a fixed length\n",
    "X = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "y = data[\"task_1\"].apply(lambda x: 1 if x == \"HOF\" else 0)      # Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "embedding_dim = 100                                            # Define the BiLSTM model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim, input_length=max_sequence_length))\n",
    "model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test))    # Train the model\n",
    "\n",
    "y_pred = model.predict(X_test)                                                            # Make predictions\n",
    "y_pred = np.round(y_pred).flatten()\n",
    "\n",
    "report = classification_report(y_test, y_pred)                                           # Calculate evaluation metrics\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
